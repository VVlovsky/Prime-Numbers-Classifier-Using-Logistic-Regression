{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twoim zadaniem jest wytrenowanie klasyfikatora binarnego na podzbiorze zbioru MNIST, w którym wyróżniamy klasy\n",
    "(cyfry 0 i 1 mają zostać wyłączone ze zbioru):\n",
    " - Liczby pierwsze (2,3,5,7)\n",
    " - Liczby złożone (4,6,8,9)\n",
    "\n",
    "Napisz wydajną implementację modelu **regresji logistycznej** trenowanego algorytmem ***SGD z momentum***.\n",
    "Cały proces trenowania musisz napisać samodzielnie, w języku Python, korzystając z biblioteki numpy. Na potrzeby zadania\n",
    "niedozwolone jest korzystanie z gotowych implementacji optimizerów i modeli oraz bibliotek do automatycznego\n",
    "różniczkowania funkcji (np. Tensorflow, pytorch, autograd).\n",
    "\n",
    "Dobierz hiperparametry tak, aby uzyskać jak najlepszy wynik na zbiorze walidacyjnym. \n",
    "Wyciągnij i zapisz wnioski z przeprowadzonych eksperymentów.\n",
    "\n",
    "Zbiór MNIST dostępny jest pod linkami: \n",
    "\n",
    "(zbiór treningowy):\n",
    " - http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
    " - http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
    "\n",
    "(zbiór walidacyjny):\n",
    " - http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
    " - http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wprowadzenie\n",
    "W doświadczeniu wykorzystałem model **regresji logistycznej** trenowanego algorytmem **SGD z momentum**.<br>\n",
    "W rezultacie przeprowadzonych badań najlepszy wynik, który udało się uzyskać na zbiorze walidacyjnym to accuracy = 97% (precision = 97% i recall = 97%). To znaczy że uzyskany model poprawnie klasyfikuje podaną na wejście cyfrę w 97% przypadkach.\n",
    "\n",
    "\n",
    "Zacznijmy od importu potzrebnych w doświadczeniu bibliotek i wczytywania danych.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import gzip as gz\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def x_create(data):\n",
    "    IMAGE_SIZE = 28\n",
    "    data.read(16)\n",
    "    buf = data.read()\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32) / 255\n",
    "    NUM_IMAGES = len(data)//IMAGE_SIZE ** 2\n",
    "    data = data.reshape(NUM_IMAGES, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    return data.tolist()\n",
    "\n",
    "\n",
    "def y_create(data):\n",
    "    data.read(8)\n",
    "    buf = data.read()\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels.tolist()\n",
    "\n",
    "\n",
    "notebook_path = os.path.abspath(\".\") + '/'\n",
    "x_train = gz.open(notebook_path + 'train-images-idx3-ubyte.gz', 'r')\n",
    "y_train = gz.open(notebook_path + 'train-labels-idx1-ubyte.gz', 'r')\n",
    "x_test = gz.open(notebook_path + 't10k-images-idx3-ubyte.gz', 'r')\n",
    "y_test = gz.open(notebook_path + 't10k-labels-idx1-ubyte.gz', 'r')\n",
    "\n",
    "data_raw = x_create(x_train)\n",
    "y_train = y_create(y_train)\n",
    "\n",
    "data_test = x_create(x_test)\n",
    "y_test = y_create(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Po czym usuwamy ze zbioru cyfry 0 i 1 i przekształcamy zbiór odpowiedzi na potrzebny do doświadczenia, t.z. jeżeli cyfra należy do zbioru liczb pierwszych to w wektorze y musi być **1**, a jeżeli nie - **0**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def data_clean(x, y):\n",
    "    to_pop = []\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 0 or y[i] == 1:\n",
    "            to_pop.append(i)\n",
    "    for i in range(len(to_pop)):\n",
    "        to_pop[i] = to_pop[i] - i\n",
    "    for p in to_pop:\n",
    "        x.pop(p)\n",
    "        y.pop(p)\n",
    "\n",
    "\n",
    "def is_prime(x):\n",
    "    return 1 if x in [2, 3, 5, 7] else 0\n",
    "\n",
    "\n",
    "data_clean(data_raw, y_train)\n",
    "data_clean(data_test, y_test)\n",
    "\n",
    "y_train = np.array(list(map(is_prime, y_train)))\n",
    "y_test = np.array(list(map(is_prime, y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Vector\n",
    "\n",
    "Oczywiście bezpośrednio podając normalizowane znaczenia każdego z pikseli jako feature vector nie otrzymamy potrzebnej informacji dla poprawnej klasyfikacji. Dodatkowo wektor o rozmiarze 28^2 elementów będzie miał duży wpływ na wydajność modelu. Więc dla mnie najcięższą częścią zadania okazało się tworzenie feature vector takiego, który dawałby dobre wyniki **precision** i **recall** i miałby nie więcej niż kilkadziesiąt elementów. \n",
    "\n",
    "Okazało się, że standardowe metody tworzenia feature vector dla klasyfikacji każdej pojedynczej cyfry są słabo efektywne dla binarnej klasyfikacji na cyfry proste / złożone.<br>\n",
    "\n",
    "Na potrzeby zadania został wymyślony algorytm **Blind Area Seach** specyficzny dla podanego zagadnienia. Polega on na \"farbowaniu\" obrazu pod różnymi kątami różnymi kolorami. I później ilość pofarbowanych pikseli wystąpia osobnym znaczeniem dla każdego koloru w feature vector. (bardziej szczegółowo niżej)<br>\n",
    "\n",
    "Chociaż ten algorytm jest podstawą działania generacji wektoru dając najwięcej informacji o klasie cyfry, jednak go samego nie wystarcza dla wysokiej dokładności. Więc trzeba było dodać dodatkowe charakterystyki dla zwiększenia dokładności, co jednak miało wpływ na wydajność modelu. Finalnie dla każdej cyfry jest tworzony wektor o rozmiarze 70 elementów.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def blind_area_search(img):\n",
    "    \n",
    "    bas_list = np.where(np.copy(img) < 0.54, 0, 1)\n",
    "    shape_size = img.shape[0]\n",
    "\n",
    "    for i in range(shape_size):\n",
    "        j = 0\n",
    "        j_r = shape_size-1\n",
    "        while j < shape_size and bas_list[i][j] == 0:\n",
    "            bas_list[i][j] = -1\n",
    "            if i > 0 and bas_list[i-1][j] == 0:\n",
    "                i_tmp = i-1\n",
    "                while bas_list[i_tmp][j] == 0 and i_tmp > 0:\n",
    "                    bas_list[i_tmp][j] = -2\n",
    "                    i_tmp -= 1\n",
    "            j += 1\n",
    "        while j_r >= 0 and bas_list[i][j_r] == 0:\n",
    "            bas_list[i][j_r] = -1\n",
    "            if i > 0 and bas_list[i-1][j_r] == 0:\n",
    "                i_tmp = i-1\n",
    "                while bas_list[i_tmp][j_r] == 0 and i_tmp > 0:\n",
    "                    bas_list[i_tmp][j_r] = -3\n",
    "                    i_tmp -= 1\n",
    "            j_r -= 1\n",
    "\n",
    "    for i in range(shape_size):\n",
    "        j = 5\n",
    "        j_r = shape_size-1\n",
    "        while j < shape_size and bas_list[i][j] == -1:\n",
    "            if i < shape_size-1 and bas_list[i+1][j] == 0:\n",
    "                i_tmp = i+1\n",
    "                while i_tmp < shape_size and bas_list[i_tmp][j] == 0:\n",
    "                    bas_list[i_tmp][j] = -4\n",
    "                    i_tmp += 1\n",
    "            j += 1\n",
    "\n",
    "\n",
    "    bas_counter = Counter(np.array(bas_list).reshape(shape_size ** 2))\n",
    "    feature_1 = bas_counter[-3]\n",
    "    feature_2 = bas_counter[-2]\n",
    "    feature_3 = bas_counter[-4]\n",
    "    feature_4 = bas_counter[0.0]\n",
    "\n",
    "    return feature_1, feature_2, feature_3, feature_4\n",
    "\n",
    "\n",
    "def cut(data_tmp):\n",
    "    data = data_tmp.copy()\n",
    "    for d in range(len(data)):\n",
    "        img = data[d]\n",
    "        cut_arr = []\n",
    "        for i in range(4):\n",
    "            cut_arr.append(cut_edge(img))\n",
    "            img = np.rot90(img)\n",
    "        for i in range(2, 4):\n",
    "            if cut_arr[i] is not None:\n",
    "                cut_arr[i] = -cut_arr[i]\n",
    "        data[d] = img[cut_arr[0]:cut_arr[2], cut_arr[1]:cut_arr[3]]\n",
    "    return data\n",
    "\n",
    "\n",
    "def cut_edge(data):\n",
    "    x = 0\n",
    "    while sum(data[x]) == 0:\n",
    "        x += 1\n",
    "    return x if x != 0 else None\n",
    "\n",
    "\n",
    "def centre(img):\n",
    "    xdent = 0\n",
    "    sor = 0\n",
    "    ydent = 0\n",
    "    for i in range(len(img)):\n",
    "        for j in range(len(img[i])):\n",
    "            xdent += img[i][j]*i\n",
    "            ydent += img[i][j]*j\n",
    "            sor += img[i][j]\n",
    "    x = xdent // sor\n",
    "    y = ydent // sor\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def vector_gen(data_raw, fast_mod=False):\n",
    "    feature_vector = []\n",
    "\n",
    "    if not fast_mod:\n",
    "        cutted = cut(data_raw)\n",
    "        for img in cutted:\n",
    "            x, y = centre(img)\n",
    "            y_rel = y / len(img[int(x)])\n",
    "            img = np.array(img)\n",
    "            size = len(img) * len(img[0])\n",
    "            fe1 = [1, y_rel]\n",
    "\n",
    "            pix_per = np.mean([img[i][j] for i in range(len(img))\n",
    "                            for j in range(len(img[0]))])\n",
    "\n",
    "            for i in range(0, 3):\n",
    "                fe1.append(np.mean([img[x][y]\n",
    "                                    for x in range(len(img)//3 * i, len(img)//3 * (i+1))\n",
    "                                    for y in range(len(img[0]))]) / pix_per)\n",
    "            for i in range(0, 3):\n",
    "                fe1.append(np.mean([img[y][x]\n",
    "                                    for x in range(len(img[0])//3 * i, len(img[0])//3 * (i+1))\n",
    "                                    for y in range(len(img))]) / pix_per)       \n",
    "            feature_vector.append(fe1)\n",
    "    else:\n",
    "        for _ in range(len(data_raw)):\n",
    "            feature_vector.append([1])\n",
    "       \n",
    "\n",
    "    for img_i in range(len(data_raw)):\n",
    "        img = np.array(data_raw[img_i])\n",
    "        pix_count = 0\n",
    "        shape_size = 28\n",
    "        for i in range(shape_size):\n",
    "            for j in range(shape_size):\n",
    "                if img[i][j] != 0:\n",
    "                    pix_count += 1\n",
    "\n",
    "        pix_per = pix_count / 784\n",
    "       \n",
    "        if not fast_mod:\n",
    "            for i in range(0, 28, 4):\n",
    "                for j in range(0, 28, 4):\n",
    "                    feature_vector[img_i].append(np.mean([img[x][y]\n",
    "                                                        for x in range(i, i+4)\n",
    "                                                        for y in range(j, j+4)]) / pix_per)\n",
    "\n",
    "        feature_vector[img_i].extend([x for x in blind_area_search(img)])\n",
    "       \n",
    "        for i in range(0, 4):\n",
    "            feature_vector[img_i].append(np.mean([img[x][y]\n",
    "                                                  for x in range(len(img)//4 * i, len(img)//4 * \n",
    "                                                  (i+1))\n",
    "                                                  for y in range(len(img[0]))]) / pix_per)\n",
    "        for i in range(0, 4):\n",
    "            feature_vector[img_i].append(np.mean([img[y][x]\n",
    "                                                  for x in range(len(img[0])//4 * i, len(img[0])\n",
    "                                                  //4 * (i+1))\n",
    "                                                  for y in range(len(img))]) / pix_per)\n",
    "\n",
    "        feature_vector[img_i].append(\n",
    "            np.mean((img[:14, :] - img[14:, :]).ravel()))\n",
    "\n",
    "    return feature_vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I teraz tworzymy zbiór wektorów opisujących obrazy:<br>\n",
    "*może to zająć około 10 minut<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(vector_gen(data_raw))\n",
    "x_test = np.array(vector_gen(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Prosty model **regresji logistycznej** trenowanego algorytmem **SGD z momentum**, dodatkowo posiadający regularyzacje L2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model:\n",
    "\n",
    "    def __init__(self, alpha=0.2, momentum_rate=0.5,\n",
    "                 max_iter=1000, l2_rate=0.01,\n",
    "                 threshold=0.5) -> None:\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.momentum_rate = momentum_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.l2_rate = l2_rate\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def sigmoid(self, X: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "\n",
    "    def gradient(self, X: np.ndarray, y: np.ndarray, beta: np.ndarray) -> np.ndarray:\n",
    "        m = y.size \n",
    "        h = self.sigmoid(X @ beta)\n",
    "        grad = (1 / m) * (X.T @ (h - y))\n",
    "        l2_grad = (self.l2_rate / m) * beta\n",
    "        return grad + l2_grad\n",
    "        \n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        self.beta = np.zeros(X.shape[1])\n",
    "        momentum = np.ones_like(self.beta)\n",
    "\n",
    "        for epoch in range(self.max_iter):\n",
    "            grad = self.gradient(X, y, self.beta)\n",
    "            momentum = self.momentum_rate * momentum - self.alpha * grad\n",
    "            self.beta = self.beta + momentum\n",
    "            \n",
    "            if epoch % 1000 == 0 and epoch != 0:\n",
    "                print(\n",
    "                    f\"---- epoch {epoch}/{self.max_iter} ----\")\n",
    "\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.int8(self.sigmoid(np.dot(X, self.beta)) >= self.threshold)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        true_positives = false_positives = true_negatives = false_negatives = 0\n",
    "\n",
    "        for predict, y_i in zip(y_pred, y_true):\n",
    "            if y_i == 1 and predict >= 0.5:  \n",
    "                true_positives += 1\n",
    "            elif y_i == 1:                   \n",
    "                false_negatives += 1\n",
    "            elif predict >= 0.5:             \n",
    "                false_positives += 1\n",
    "            else:                            \n",
    "                true_negatives += 1\n",
    "\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "        accuracy = (true_positives + true_negatives) / len(y_pred)\n",
    "\n",
    "        return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalny etap - tworzymy obiekt modelu i patrzymy na wyniki :)<br>\n",
    "*10000 epoch to dość dużo i zajmuje kilka minut. Można byłoby uzyskać podobny wynik accuracy dużo szybciej, jednak uzyskać zbalansowane precision i recall udało mi się tylko przy takiej ilości iteracji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(max_iter=10000, momentum_rate=0.18, alpha=0.2)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print(model.evaluate(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ewentualnie można zobaczyć które obrazy nie udało się poprawnie sklasyfikować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_ident_arr = []\n",
    "for i in range(500):\n",
    "    if y_pred[i] != y_test[i]:\n",
    "        not_ident_arr.append(i)\n",
    "\n",
    "f, axarr = plt.subplots(3,3)\n",
    "axarr[0,2].imshow(np.asarray(data_test[not_ident_arr[2]]).squeeze())\n",
    "axarr[1,0].imshow(np.asarray(data_test[not_ident_arr[3]]).squeeze())\n",
    "axarr[1,1].imshow(np.asarray(data_test[not_ident_arr[4]]).squeeze())\n",
    "axarr[1,2].imshow(np.asarray(data_test[not_ident_arr[5]]).squeeze())\n",
    "axarr[2,0].imshow(np.asarray(data_test[not_ident_arr[6]]).squeeze())\n",
    "axarr[2,1].imshow(np.asarray(data_test[not_ident_arr[7]]).squeeze())\n",
    "axarr[2,2].imshow(np.asarray(data_test[not_ident_arr[8]]).squeeze())\n",
    "axarr[0,0].imshow(np.asarray(data_test[not_ident_arr[9]]).squeeze())\n",
    "axarr[0,1].imshow(np.asarray(data_test[not_ident_arr[11]]).squeeze())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podsumowanie \n",
    "Uważam 97% bardzo dobrym wynikiem (ponieważ niektóre z cyfr ja sam nie mogę sklasyfikować poprawnie :D), a co do czasu wykonania myślę że wydajność wszystkiego procesu tworzenia chociaż i nie jest najlepsza, jednak leży w możliwych przedziałach.<br>\n",
    "Ogólnie da się zastosować algorytm PCA żeby zredukować liczbę punktów informacji, jednak taka operacja może zmniejszyć dokładność modelu. Ze względu na to zdecydowałem nie korzystać się z czegoś takiego ponieważ uważam że w tym doświadczeniu dopóki czas na działanie algorytmu leży w przedziałach kilku minut najważniejsze jest polepszenie dokładności.<br>\n",
    "\n",
    "\n",
    "Dziękuję za uwagę, będę bardzo wdzięczny za informację zwrotną (blędy, uwagi, etc.) :)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('main': conda)",
   "language": "python",
   "name": "python38264bitmainconda02a2c8cc71554804b3aa4a6cc8fe87bc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
